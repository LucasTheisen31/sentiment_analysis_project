{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pip6fW4wZh_0"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWy89VOulC1c"
      },
      "source": [
        "Recomendações de Hiperparâmetros para o Ajuste Fino (Fine-tuning) do BERT\n",
        "Recomendações dos autores do BERT para a escolha dos principais hiperparâmetros durante o processo de ajuste fino, conforme descrito no artigo original:\n",
        "\n",
        "Tamanho do Lote (Batch size): 16 ou 32.\n",
        "\n",
        "Taxa de Aprendizagem (Learning rate): Para o otimizador AdamW, valores ideais geralmente estão na faixa:\n",
        "\n",
        "2e-5, 3e-5, 5e-5\n",
        "\n",
        "Número de Épocas (Epochs): O treinamento é tipicamente executado por 2, 3 ou 4 épocas, pois um número maior pode levar a um overfitting rapidamente.\n",
        "\n",
        "É crucial notar a relação inversa entre o tamanho do lote e a precisão do modelo. Embora lotes maiores possam acelerar o tempo de treinamento (utilizando mais a capacidade da GPU), eles podem levar a uma precisão ligeiramente inferior, pois o modelo tem menos oportunidades de atualizar seus pesos com base em diferentes subconjuntos de dados. Para otimizar a generalização, lotes menores (como 16) são frequentemente preferidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bAZvEoGZh_2",
        "outputId": "4d9897af-f547-4876-c978-4815097a36ee"
      },
      "outputs": [],
      "source": [
        "# Instalar PyTorch, TorchVision e Torchaudio\n",
        "%pip install torch torchvision torchaudio\n",
        "\n",
        "# Instalar outras bibliotecas necessárias\n",
        "%pip install transformers==4.29.2 numpy pandas seaborn matplotlib scikit-learn\n",
        "\n",
        "# Instalar o watermark para exibir versões das bibliotecas\n",
        "%pip install -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui9s2LUgZh_3",
        "outputId": "837bd705-7d69-4a8c-ca3b-ca0c739f0868"
      },
      "outputs": [],
      "source": [
        "%reload_ext watermark\n",
        "\n",
        "# Exibir versões das bibliotecas\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWcVv1KcZh_4",
        "outputId": "caa0c43e-9815-4cb4-9e1b-a4b38be28378"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni97RmHKZh_4"
      },
      "outputs": [],
      "source": [
        "# A partir da biblioteca Transformers da Hugging Face, importando as classes\n",
        "import transformers\n",
        "from transformers import AutoModel # ou AutoModel, para carregar o modelo BERT\n",
        "from transformers import AutoTokenizer  # ou AutoTokenizer, para carregar o tokenizador BERT\n",
        "from transformers import get_linear_schedule_with_warmup  # Para controle do agendamento da taxa de aprendizado\n",
        "\n",
        "# Importando a biblioteca PyTorch\n",
        "import torch\n",
        "from torch.optim import AdamW # Para otimização do modelo\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from collections import defaultdict\n",
        "from textwrap import wrap\n",
        "\n",
        "import os\n",
        "# from google.colab import files  # Específico do Google Colab\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "sns.set(style='whitegrid', palette='viridis', font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "rcParams['figure.figsize'] = 12, 8\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Modelos\n",
        "BERTimbauBase = 'neuralmind/bert-base-portuguese-cased'\n",
        "BERTimbauLarge = 'neuralmind/bert-large-portuguese-cased'\n",
        "BERMultilingualBase = 'google-bert/bert-base-multilingual-cased' # Modelo BERT multilíngue do Google\n",
        "XLMRobertaBase = 'FacebookAI/xlm-roberta-base' # Modelo XLM-RoBERTa é uma versão multilíngue do RoBERTa.\n",
        "\n",
        "PRE_TRAINED_MODEL_NAME = BERTimbauBase\n",
        "\n",
        "# Carregando o modelo BERTimbau da Hugging Face pré-treinado em português\n",
        "base_model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n",
        "\n",
        "# Carregando o tokenizador BERTimbau da Hugging Face pré-treinado em português\n",
        "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "# Configuração de treino\n",
        "EPOCHS = 10\n",
        "# Teste e validação terão cada um a metade do TEST_SIZE\n",
        "TEST_SIZE = 0.3\n",
        "# Taxa de dropout para regularização, ajudando a prevenir o sobreajuste (overfitting).\n",
        "DROPOUT = 0.3\n",
        "# O parâmetro WEIGHT_DECAY=0.01 ajuda a reduzir overfitting penalizando pesos muito grandes.\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# Tamanho do lote \"BATCH\" (Quantos exemplos um conjunto de dados vai ter) # 4, 8, 16\n",
        "BATCH_SIZE = 16\n",
        "# Taxa de aprendizado (Learning Rate) para o otimizador # 2e-5, 3e-5 ou 5e-5\n",
        "LEARNING_RATE = 3e-5\n",
        "# Habilita ou desabilita o uso de pesos para balanceamento de classes\n",
        "CLASS_WEIGHTS = True\n",
        "\n",
        "dataset_name = 'reviews_3000_limpeza_demojize.csv'\n",
        "score_column = 'score'\n",
        "sentiment_column = 'sentiment'\n",
        "\n",
        "class_names = [\n",
        "        'extremamente negativo',\n",
        "        'negativo',\n",
        "        'neutro',\n",
        "        'positivo',\n",
        "        'extremamente positivo',\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0Jz39UkWUuc",
        "outputId": "4e2b6279-6528-46ca-f772-ff736315ca74"
      },
      "outputs": [],
      "source": [
        "print(f\"Classe do modelo carregado: {type(base_model)}\")\n",
        "print(f\"Classe do tokenizer carregado: {type(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeCRg5ILZh_5",
        "outputId": "4874a2d2-73a3-4c63-b01d-6347d7bf0e94"
      },
      "outputs": [],
      "source": [
        "# Verificando se a GPU com suporte a CUDA está disponível no sistema\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTReEvLdZh_5",
        "outputId": "56153483-79e1-46f6-eaaf-9b8ad4f3da53"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0SnFrW9Zh_5"
      },
      "source": [
        "## Manipulação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5twfPvqZh_5"
      },
      "source": [
        "## Importação dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrnOLQOgEc8W"
      },
      "outputs": [],
      "source": [
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "GsM1QIInZh_5",
        "outputId": "1aa0fa85-d528-4c01-f03f-c714063e80ff"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset_name)\n",
        "\n",
        "\n",
        "def get_sentiment(rating):\n",
        "    rating = int(rating)\n",
        "    if rating == 1:\n",
        "        return 0\n",
        "    elif rating == 2:\n",
        "        return 1\n",
        "    elif rating == 3:\n",
        "        return 2\n",
        "    elif rating == 4:\n",
        "        return 3\n",
        "    else:\n",
        "        return 4\n",
        "\n",
        "\n",
        "df[sentiment_column] = df[score_column].apply(get_sentiment)\n",
        "\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N844A4JZh_5",
        "outputId": "557fac8c-793c-45c8-be4c-9f77760eb952"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGmQBX6yZh_6",
        "outputId": "4a6feb9b-6e89-4252-a135-af03e6351285"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "3NjaLhRQZh_6",
        "outputId": "f5b11523-537d-4435-983d-4e8fd16c3a9a"
      },
      "outputs": [],
      "source": [
        "df.sentiment.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "tM2zVpb_Zh_6",
        "outputId": "2b7f78e0-d8eb-4c3e-88e9-583429af1132"
      },
      "outputs": [],
      "source": [
        "# Contar as avaliações por score\n",
        "sentiment_counts = df.sentiment.value_counts()\n",
        "\n",
        "# Plotar as contagens por sentimento\n",
        "sns.barplot(x=class_names, y=sentiment_counts.values, palette=\"viridis\")\n",
        "\n",
        "plt.xlabel(\"Sentimento\")\n",
        "plt.ylabel(\"Número de Avaliações\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qjLd1ApZh_6"
      },
      "source": [
        "## Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3OJs6eQZh_6"
      },
      "source": [
        "Antes de alimentar textos para o modelo BERT, é necessário realizar algumas etapas de pré-processamento. O BERT requer um formato específico de entrada, e uma das etapas essenciais é a criação de vetores de 0s e 1s chamados attention mask, que indicam quais tokens devem ser considerados válidos, e a adição de três tokens especiais aos textos:\n",
        "\n",
        "* [SEP] (102)- Marca o fim de uma frase\n",
        "* [CLS] (101)- Deve ser colocado no inicio de cada frase para o BERT saber que trata-se de um problema de classificação.\n",
        "* [PAD] (0)- Tokens de valor 0 que devem ser adicionados às sentenças para garantir que todas tenham o mesmo tamanho.\n",
        "\n",
        "Esses tokens podem ser adicionados utilizando o método [encode_plus](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode_plus) da Hugging Face. Esse método realiza várias operações essenciais, como:\n",
        "\n",
        "* Tokenizar o texto: Converte as palavras em IDs de tokens que correspondem ao vocabulário do modelo BERT.\n",
        "Adicionar tokens especiais: Insere automaticamente os tokens [CLS], [SEP] e [PAD] quando necessário.\n",
        "* Realizar padding (preenchimento): Preenche as sentenças mais curtas com tokens [PAD] até atingir o comprimento máximo especificado.\n",
        "* Gerar máscara de atenção (attention mask): Cria uma máscara que indica quais tokens são reais (com valor 1) e quais são de preenchimento (com valor 0), ajudando o modelo a focar nos tokens importantes.\n",
        "* Controle de truncamento: Garante que textos mais longos sejam truncados para o comprimento máximo permitido.\n",
        "* Retornar tensores: Converte os resultados para tensores PyTorch ou TensorFlow, prontos para uso em modelos de aprendizado de máquina.\n",
        "\n",
        "1. **Tokens Especiais:**\n",
        "* [CLS] (101): Deve ser colocado no início de cada frase para que o BERT saiba que trata-se de um problema de classificação.\n",
        "* [SEP] (102): Marca o fim de uma frase ou sequência.\n",
        "* [PAD] (0): Tokens de valor 0 que devem ser adicionados às sentenças para garantir que todas tenham o mesmo tamanho (padding).\n",
        "[UNK] (100): Token utilizado para palavras desconhecidas, que não foram vistas durante o treinamento. Ex:\n",
        " * 101 -> [CLS]\n",
        " * 2146 -> \"O\"\n",
        " * 1004 -> \"BERT\"\n",
        " * 2003 -> \"é\"\n",
        " * 1037 -> \"uma\"\n",
        " * 4600 -> \"ferramenta\"\n",
        " * 2395 -> \"poderosa\"\n",
        " * 102 -> [SEP]\n",
        "\n",
        "2. **Padding (Preenchimento):**\n",
        "* O BERT espera que todas as entradas tenham o mesmo comprimento. Isso significa que você precisará preencher (pad) as sentenças mais curtas com tokens de preenchimento (como 0) até um comprimento fixo. Isso é necessário para processamento em lotes (batches) eficiente. Ex:\n",
        " * 101 -> [CLS]\n",
        " * 2146 -> \"O\"\n",
        " * 1004 -> \"BERT\"\n",
        " * 2003 -> \"é\"\n",
        " * 1037 -> \"uma\"\n",
        " * 4600 -> \"ferramenta\"\n",
        " * 2395 -> \"poderosa\"\n",
        " * 102 -> [SEP]\n",
        " * 0 -> [PAD]\n",
        " * 0 -> [PAD]\n",
        " * 0 -> [PAD]\n",
        " * 0 -> [PAD]\n",
        "\n",
        "3. **Máscara de Atenção (Attention Mask):**\n",
        "* A máscara de atenção é uma matriz que indica quais tokens são reais (com valor 1) e quais são de preenchimento (com valor 0). Isso ajuda o BERT a se concentrar nos tokens importantes e ignorar os preenchimentos. Ex:\n",
        " * 101 -> 1\n",
        " * 2146 -> 1\n",
        " * 1004 -> 1\n",
        " * 2003 -> 1\n",
        " * 1037 -> 1\n",
        " * 4600 -> 1\n",
        " * 2395 -> 1\n",
        " * 102 -> 1\n",
        " * 0 -> 0\n",
        " * 0 -> 0\n",
        " * 0 -> 0\n",
        " * 0 -> 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNOHSBhnZh_6"
      },
      "source": [
        "#### Exemplo do processo de tokenização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1SPEaBbZh_6",
        "outputId": "9cdc3d3f-6d8c-4a16-a2be-4630eec2d16f"
      },
      "outputs": [],
      "source": [
        "sample_txt = \"Santa Helena é uma cidade\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f\" Sentence: {sample_txt}\")\n",
        "print(f\"   Tokens: {tokens}\")\n",
        "print(f\"Token IDs: {token_ids}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aQEPbBmZh_7",
        "outputId": "793079ea-c9de-4892-b0f4-a9dc94f3e4b9"
      },
      "outputs": [],
      "source": [
        "# [SEP] - Marcador para indicar o fim de uma sentença.\n",
        "tokenizer.sep_token, tokenizer.sep_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCxxqc6pZh_7",
        "outputId": "35b3ab6b-a838-4b0a-bc35-4838eeefae99"
      },
      "outputs": [],
      "source": [
        "# [CLS] - Token que deve ser adicionado no início de cada sentença.\n",
        "tokenizer.cls_token, tokenizer.cls_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YURC5jPwZh_7",
        "outputId": "6ddf3606-2db0-4bd8-b438-01d14b2eff2f"
      },
      "outputs": [],
      "source": [
        "# [PAD] - Token de preenchimento utilizado para garantir que todas as sentenças tenham o mesmo tamanho (padding).\n",
        "tokenizer.pad_token, tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umEU4u6vZh_7",
        "outputId": "2822da89-8d10-4837-c186-5eb63622e6b3"
      },
      "outputs": [],
      "source": [
        "# [UNK] - Token de palavras desconhecidas, usadas para palavras que não apareceram no conjunto de treinamento do modelo.\n",
        "tokenizer.unk_token, tokenizer.unk_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpUf_IiXZh_7"
      },
      "source": [
        "Todo esse trabalho pode ser feito usando o método [encode_plus](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIiZn7VdZh_7",
        "outputId": "311e6ff7-30a7-4bed-8999-f7967c060e2b"
      },
      "outputs": [],
      "source": [
        "# Usa o método encode_plus() para preparar os dados para o modelo\n",
        "encoding = tokenizer.encode_plus(\n",
        "    sample_txt,  # Texto a ser tokenizado\n",
        "    add_special_tokens=True,  # Adiciona os tokens especiais [CLS] no início e [SEP]\n",
        "    max_length=9,  # Define o comprimento máximo da sequência \"entrada\" (160 tokens \"palavras\" neste caso)\n",
        "    truncation=True,  # Trunca a sequência se ultrapassar o comprimento máximo\n",
        "    padding=\"max_length\",  # Realiza o padding, adicionando tokens de preenchimento [PAD] até o comprimento máximo\n",
        "    return_token_type_ids=False,  # Não retorna os 'token_type_ids', pois não são necessários para analise de sentimentos\n",
        "    return_attention_mask=True,  # Retorna a máscara de atenção, indicando quais tokens devem ser considerados (1 para tokens reais, 0 para padding)\n",
        "    return_tensors=\"pt\",  # Retorna os resultados em formato de tensor PyTorch\n",
        ")\n",
        "\n",
        "# O dicionário resultante contém 'input_ids' (sequência de tokens) e 'attention_mask' (máscara de atenção)\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r7JrzkUZh_7",
        "outputId": "39a9aede-e002-44fa-be46-8cc4dded50ff"
      },
      "outputs": [],
      "source": [
        "encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BNeqep-Zh_7",
        "outputId": "2dc0f28c-fda0-4783-aa5c-781d3ac7f163"
      },
      "outputs": [],
      "source": [
        "# Os IDs dos tokens agora são armazenados em um Tensor e preenchidos até um comprimento de 160 colunas\n",
        "print(\n",
        "    len(encoding[\"input_ids\"][0])\n",
        ")  # Exibe o comprimento da sequência de tokens (esperado 160, incluindo tokens especiais e padding)\n",
        "# Exibe o tensor contendo os IDs dos tokens, incluindo o [CLS], [SEP] e tokens de preenchimento [PAD]\n",
        "encoding[\"input_ids\"]\n",
        "\n",
        "# A máscara de atenção tem o mesmo comprimento:\n",
        "print(len(encoding[\"attention_mask\"][0]))\n",
        "# Exibe a máscara de atenção (1 para tokens reais, 0 para tokens de padding)\n",
        "encoding[\"attention_mask\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HMwBjFUZh_8",
        "outputId": "cd00794c-045f-41c5-c9ca-2e6119859ee4"
      },
      "outputs": [],
      "source": [
        "# Converte os IDs dos tokens de volta para tokens\n",
        "tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP3nYLmCZh_8"
      },
      "source": [
        "##### Exemplo: Codificação do nosso texto de exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZQJCjULZh_8"
      },
      "outputs": [],
      "source": [
        "# Passa os tokens de entrada e a máscara de atenção para o modelo BERT\n",
        "last_hidden_state, pooled_output = base_model(\n",
        "    input_ids=encoding[\n",
        "        \"input_ids\"\n",
        "    ],  # IDs dos tokens, gerados pelo tokenizer a partir do texto de entrada\n",
        "    attention_mask=encoding[\"attention_mask\"],  # Máscara de atenção\n",
        ")\n",
        "\n",
        "# last_hidden_state é a saída da última camada oculta do modelo BERT\n",
        "# Ele contém as representações de cada token no contexto da frase inteira\n",
        "# Forma: [batch_size, sequence_length, hidden_size]\n",
        "# Ex: Para um modelo BERT Base, a dimensão será [batch_size, 768]\n",
        "\n",
        "# pooled_output é a saída correspondente ao token [CLS] (usado para tarefas de classificação)\n",
        "# É como um resumo da frase inteira, ou seja, é uma única representação para a frase\n",
        "# Geralmente, usado para tarefas de classificação\n",
        "# Forma: [batch_size, hidden_size]\n",
        "# Ex: Para BERT Base, será [batch_size, 768]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcQfmiuCZh_8",
        "outputId": "b154dca1-88d0-4307-9923-4611d13412a6"
      },
      "outputs": [],
      "source": [
        "# Forma: [batch_size, sequence_length, hidden_size]\n",
        "# Last_hidden_state é a saída da última camada oculta do modelo BERT, contendo as representações de cada token no contexto da frase inteira\n",
        "last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGPQMM5pZh_8",
        "outputId": "d4c0f05d-a4dc-4b2d-df95-c9d5295dd736"
      },
      "outputs": [],
      "source": [
        "# Forma: [batch_size, hidden_size]\n",
        "# Lembrando que pooled_output é a saída correspondente ao token [CLS], que é uma representação da frase inteira, usada para classificação\n",
        "pooled_output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jonJ88uPZh_8"
      },
      "source": [
        "O `last_hidden_state` é uma sequência de estados ocultos da última camada do modelo. A obtenção do `pooled_output` é feita aplicando o [BertPooler](https://github.com/huggingface/transformers/blob/edf0582c0be87b60f94f41c659ea779876efc7be/src/transformers/modeling_bert.py#L426) em `last_hidden_state`.\n",
        "\n",
        "`pooled_output` é a saída correspondente ao token [CLS] (usado para tarefas de classificação)\n",
        "Podemos pensar como um resumo da frase inteira, ou seja, é uma única representação para a frase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pjBdkJbZh_8",
        "outputId": "e15bca6b-1c85-4eb3-e6e1-8f0311781413"
      },
      "outputs": [],
      "source": [
        "# Obtém o tamanho do vetor de características ocultas (hidden_size) configurado no modelo BERT\n",
        "# Para BERT Base, o valor será 768. Para BERT Large, será 1024.\n",
        "base_model.config.hidden_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8uJ4UA2Zh_8"
      },
      "source": [
        "#### Escolhendo o comprimento da sequência"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0CdOGqmZh_8"
      },
      "source": [
        "BERTimbau Base e BERTimbau Large: as entradas podem ter até 512 palavras (tokens)\n",
        "\n",
        "Para escolher o comprimento máximo de sequência, vamos armazenar o número de tokens de cada review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F_sguqOZh_9"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in df.content:\n",
        "    tokens = tokenizer.encode(txt, truncation=True, max_length=512)\n",
        "    token_lens.append(len(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXhi5oA5Zh_9"
      },
      "source": [
        "Exibir a distribuição"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "yf9VMTPeZh_9",
        "outputId": "31490903-89a5-4d1c-9726-e0b927d5021a"
      },
      "outputs": [],
      "source": [
        "sns.histplot(token_lens, bins=30, kde=True)  # kde=True adiciona a curva de densidade\n",
        "\n",
        "# Define os rótulos e o título\n",
        "plt.xlabel(\"Contagem de Tokens\")\n",
        "plt.ylabel(\"Frequência\")\n",
        "plt.title(\"Distribuição do Número de Tokens\")\n",
        "\n",
        "# Encontra e exibe o número máximo de tokens no gráfico\n",
        "MAX_LEN = max(token_lens)\n",
        "plt.axvline(MAX_LEN, color=\"r\", linestyle=\"--\", label=f\"Máximo de Tokens: {MAX_LEN}\")\n",
        "\n",
        "# Adiciona uma legenda para explicar a linha vermelha\n",
        "plt.legend()\n",
        "\n",
        "# Salva a imagem em um arquivo\n",
        "plt.savefig(\"distribuicao_tokens.png\")\n",
        "\n",
        "# Exibe o gráfico (opcional, dependendo do seu ambiente de execução)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYBZh006Zh_9",
        "outputId": "002a0a95-5239-4b2f-a5a4-593e19b6e3fa"
      },
      "outputs": [],
      "source": [
        "max_len_val = max(token_lens)\n",
        "\n",
        "MAX_LEN = 512 if max_len_val >= 512 else max_len_val\n",
        "\n",
        "print(f\"O comprimento máximo de tokens é: {MAX_LEN}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asD3lvetZh_-"
      },
      "source": [
        "#### Dividindo o conjunto de dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxQ3jR-aZh_-",
        "outputId": "3ca3d363-0206-4c89-8671-f9dc8916b0e0"
      },
      "outputs": [],
      "source": [
        "# Divide o DataFrame em conjunto de treinamento, teste e validação\n",
        "# Primeiro divide em treino e teste\n",
        "df_train, df_test = train_test_split(df, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
        "\n",
        "# Divide o conjunto de teste em dois, criando um conjunto de validação (50% do teste) e o conjunto de teste final (50% do teste)\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
        "\n",
        "df_train.shape  # Tamanho do conjunto de treinamento\n",
        "df_val.shape  # Tamanho do conjunto de validação\n",
        "df_test.shape  # Tamanho do conjunto de teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ZMEvwLZh_-"
      },
      "source": [
        "#### Criando o Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfGYnR3WZh_-"
      },
      "outputs": [],
      "source": [
        "# Estende a classe Dataset do PyTorch\n",
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "    # Inicializa o dataset com os reviews, targets (sentimentos), tokenizador e o comprimento máximo da sequência\n",
        "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "        self.reviews = reviews  # Armazena os textos das avaliações\n",
        "        self.targets = targets  # Armazena os sentimentos das avaliações\n",
        "        self.tokenizer = tokenizer  # Tokenizador BERT\n",
        "        self.max_len = max_len  # Comprimento máximo da sequência\n",
        "\n",
        "    # Método __len__ retorna o número de exemplos no dataset (Númeor de reviews)\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    # Método __getitem__ recupera um item individual do dataset\n",
        "    def __getitem__(self, item):\n",
        "        review = str(self.reviews[item])  # Obtém o review atual e o converte em string\n",
        "        target = self.targets[item]  # Obtém o rótulo correspondente ao review atual\n",
        "\n",
        "        # Codifica o review em IDs de tokens e máscaras de atenção usando o tokenizador BERT\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            add_special_tokens=True,  # Adiciona tokens especiais como [CLS] e [SEP]\n",
        "            max_length=self.max_len,  # Define o comprimento máximo para truncamento/preenchimento\n",
        "            truncation=True,  # Trunca a sequência se ultrapassar o comprimento máximo\n",
        "            return_token_type_ids=False,  # Não retorna os IDs de tipo de token, pois não é necessário\n",
        "            padding=\"max_length\",  # Preenche as sequências até o comprimento máximo\n",
        "            return_attention_mask=True,  # Retorna a máscara de atenção que indica os tokens válidos\n",
        "            return_tensors=\"pt\",  # Retorna os tensores prontos para PyTorch\n",
        "        )\n",
        "\n",
        "        # Retorna um dicionário com o texto do review, os IDs dos tokens, a máscara de atenção e o rótulo\n",
        "        return {\n",
        "            \"review_text\": review,  # O review original em texto\n",
        "            \"input_ids\": encoding[\n",
        "                \"input_ids\"\n",
        "            ].flatten(),  # IDs dos tokens achatados em uma dimensão\n",
        "            \"attention_mask\": encoding[\n",
        "                \"attention_mask\"\n",
        "            ].flatten(),  # Máscara de atenção achatada\n",
        "            \"targets\": torch.tensor(\n",
        "                target, dtype=torch.long\n",
        "            ),  # Rótulo convertido para tensor PyTorch de tipo long\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLSjs8GxZh_-"
      },
      "source": [
        "Criar alguns carregadores de dados. Aqui está uma função auxiliar para fazer isso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfizqBrTZh_-"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size, shuffle: bool):\n",
        "    # Cria um dataset GPReviewDataset a partir dos dados de entrada\n",
        "    ds = GPReviewDataset(\n",
        "        reviews=df.content.to_numpy(),  # Converte a coluna dos reviews (content) para um array numpy\n",
        "        targets=df[\n",
        "            sentiment_column\n",
        "        ].to_numpy(),  # Converte a coluna de sentimentos para um array numpy\n",
        "        tokenizer=tokenizer,  # Tokenizador do BERT para processar os reviews\n",
        "        max_len=max_len,  # Comprimento máximo para o padding/truncamento\n",
        "    )\n",
        "\n",
        "    # Retorna um DataLoader para o dataset, que divide os dados em lotes\n",
        "    return DataLoader(\n",
        "        dataset=ds,  # O dataset criado\n",
        "        batch_size=batch_size,  # Tamanho do lote\n",
        "        # num_workers=4,           # Número de processos para usar para carregar os dados.\n",
        "        num_workers=0,  # Evita o multiprocessamento, usando um único processo para carregar os dados\n",
        "        shuffle=shuffle,  # Embaralha ou não dados a cada época para melhorar a generalização do modelo\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgtles1_e8sa"
      },
      "source": [
        "Criando DataLoaders para os conjuntos de dados de treinamento, validação e teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t3TA9PtZh_-"
      },
      "outputs": [],
      "source": [
        "# Cada DataLoader vai conter os dados em lotes, facilitando o treinamento do modelo\n",
        "# Dados de treinamento são embaralhados (shuffle = True)\n",
        "train_data_loader = create_data_loader(\n",
        "    df_train, tokenizer, MAX_LEN, BATCH_SIZE, shuffle=True\n",
        ")\n",
        "val_data_loader = create_data_loader(\n",
        "    df_val, tokenizer, MAX_LEN, BATCH_SIZE, shuffle=False\n",
        ")\n",
        "test_data_loader = create_data_loader(\n",
        "    df_test, tokenizer, MAX_LEN, BATCH_SIZE, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiyGv_iSgf7R"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPaNbpDRZh_-",
        "outputId": "156f9c3b-357f-4c4b-8f05-1c16e5ed1946"
      },
      "outputs": [],
      "source": [
        "print('Numero de lotes \"batches\" de treinamento:', len(train_data_loader))\n",
        "print('Tamanho dos lotes \"batches\" de treinamento:', train_data_loader.batch_size)\n",
        "\n",
        "print('Numero de lotes \"batches\" de validação:', len(val_data_loader))\n",
        "print('Tamanho dos lotes \"batches\" de validação:', val_data_loader.batch_size)\n",
        "\n",
        "print('Numero de lotes \"batches\" de teste:', len(test_data_loader))\n",
        "print('Tamanho dos lotes \"batches\" de teste:', test_data_loader.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-L0FdPbZh__"
      },
      "source": [
        "## Criando o Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ustpj3lZh__"
      },
      "outputs": [],
      "source": [
        "# Definindo o Classificador de Sentimento (BERT + Dropout + Camada Final) para Classificação.\n",
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "\n",
        "        # Inicializa o modelo BERT pré-treinado.\n",
        "        # O BERT será responsável por gerar as representações contextuais dos textos de entrada.\n",
        "        # O `return_dict=False` significa que o retorno será uma tupla e não um dicionário.\n",
        "        self.bert = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n",
        "\n",
        "        # Camada de Dropout para regularização durante o treinamento.\n",
        "        # Dropout é uma técnica que desativa aleatoriamente algumas conexões entre neurônios,\n",
        "        # ajudando a evitar overfitting.\n",
        "        self.drop = nn.Dropout(p=DROPOUT)\n",
        "\n",
        "        # Camada final totalmente conectada (linear), que mapeia o vetor de saída do BERT para a quantidade de classes de saída.\n",
        "        # O número de features de entrada (in_features) é o tamanho da saída do BERT (geralmente 768).\n",
        "        # O número de classes (out_features) é especificado pelo parâmetro `n_classes`, que no caso é 5 para sentimentos.\n",
        "        self.out = nn.Linear(\n",
        "            in_features=self.bert.config.hidden_size, out_features=n_classes\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # O método forward define o que acontece quando o modelo recebe dados de entrada.\n",
        "        # O modelo BERT gera uma tupla com duas saídas: sequence_output e pooled_output.\n",
        "        # A variável 'pooled_output' é a saída do token [CLS], que será usada para classificação.\n",
        "        # `input_ids`: ids dos tokens que representam o texto.\n",
        "        # `attention_mask`: mascara que indica quais tokens devem ser atendidos (ignora padding).\n",
        "\n",
        "        # `pooled_output` é a representação do token [CLS], que é utilizado para tarefas de classificação.\n",
        "        _, pooled_output = self.bert(\n",
        "            input_ids=input_ids,  # IDs dos tokens de entrada\n",
        "            attention_mask=attention_mask,  # Máscara de atenção para ignorar tokens de padding\n",
        "        )\n",
        "\n",
        "        # Aplica o Dropout sobre o pooled_output para regularização.\n",
        "        # Durante o treinamento, algumas conexões são \"desligadas\" para evitar overfitting.\n",
        "        output = self.drop(pooled_output)\n",
        "\n",
        "        # A saída do Dropout é passada pela camada totalmente conectada (linear) para gerar as previsões de classe.\n",
        "        # Aqui, `self.out(output)` retorna os logits para cada classe (por exemplo, positivo, neutro, negativo).\n",
        "        return self.out(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DK_I9N5Zh__"
      },
      "source": [
        "Usamos uma camada dropout para alguma regularização e uma camada totalmente conectada para nossa saída. Observe que estamos retornando a saída bruta da última camada, pois isso é necessário para que a função cross-entropy loss function no PyTorch funcione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNUFqg_IZh__"
      },
      "outputs": [],
      "source": [
        "# Instanciar o modelo de classificação de sentimentosmovê-lo para a GPU:\n",
        "# O número de classes é 5 (extremamente negativo, negativo, neutro, positivo, extremamente positivo).\n",
        "model = SentimentClassifier(len(class_names))\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGYL3QqQilSU"
      },
      "source": [
        "### Exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohyDofOvZh__",
        "outputId": "b253a5e5-563e-4ee5-cf04-88eb5dc81568"
      },
      "outputs": [],
      "source": [
        "# EXEMPLO\n",
        "# Obtendo o primeiro lote de dados do train_data_loader\n",
        "data = next(iter(train_data_loader))\n",
        "# Exibindo as dimensões dos tensores de um lote de dados\n",
        "print(\n",
        "    \"input_ids shape: \", data[\"input_ids\"].shape\n",
        ")  # Formato esperado: [batch_size, max_len] para os IDs dos tokens dos reviews\n",
        "print(\n",
        "    \"attention_mask shape: \", data[\"attention_mask\"].shape\n",
        ")  # Formato esperado: [batch_size, max_len] para a máscara de atenção\n",
        "print(\n",
        "    \"targets shape: \", data[\"targets\"].shape\n",
        ")  # Formato esperado: [batch_size] para os rótulos (sentimentos)\n",
        "\n",
        "# Saida [batch_size, max_len]\n",
        "# 'batch_size' refere-se ao número de amostras no lote.\n",
        "# 'max_len'  refere-se ao número máximo de tokens por amostra (É o valor de MAX_LEN que foi configurado anteriormente)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19he3exWZh__",
        "outputId": "aea1c77a-d8ae-4d11-cac4-edb530679a7d"
      },
      "outputs": [],
      "source": [
        "# EXEMPLO\n",
        "# Move os 'input_ids' e 'attention_mask' para o dispositivo correto (GPU ou CPU)\n",
        "# 'input_ids' contém os IDs dos tokens do texto de entrada, já processados pelo tokenizador do BERT\n",
        "# 'attention_mask' contém uma máscara que indica quais tokens são válidos (1) e quais são padding (0)\n",
        "input_ids = data[\"input_ids\"].to(device)\n",
        "attention_mask = data[\"attention_mask\"].to(device)\n",
        "\n",
        "## model(input_ids, attention_mask):\n",
        "# Esta parte passa os dados de entrada para o modelo (input_ids e attention_mask)\n",
        "# O modelo retorna uma saída, que é uma previsão bruta (logits) para cada classe.\n",
        "outputs = model(input_ids, attention_mask)\n",
        "\n",
        "# Aplica a função softmax nos logits para converter os valores brutos em probabilidades de classe\n",
        "# A função `softmax` garante que a soma das probabilidades seja 1 para cada exemplo no batch\n",
        "# `dim=1` indica que a operação softmax deve ser aplicada ao longo da segunda dimensão (as colunas), ou seja, sobre as classes\n",
        "# Isso faz com que as previsões se tornem probabilidades, onde a classe com a maior probabilidade será a prevista\n",
        "predictions = F.softmax(outputs, dim=1)\n",
        "\n",
        "print(\"Outputs: \", outputs)\n",
        "print(\"Predictions: \", predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAgxDxXuZh__"
      },
      "source": [
        "## Treino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqIcj2Fej_AB"
      },
      "source": [
        "Procedimento de Treinamento e Otimização:\n",
        "\n",
        "- Para replicar a metodologia de treinamento do artigo original do BERT, o processo de ajuste fino será conduzido utilizando o otimizador AdamW, fornecido pela biblioteca Hugging Face. Este otimizador é uma variante do Adam que implementa a correção do weight decay (decaimento de peso), alinhando-se com a abordagem descrita pelos autores para evitar o overfitting.\n",
        "\n",
        "- Adicionalmente, será empregado um agendador linear de taxa de aprendizado (get_linear_schedule_with_warmup), configurado sem etapas de warm-up. Este agendador ajusta a taxa de aprendizado de forma gradual ao longo das épocas, contribuindo para uma convergência mais estável e eficiente do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIVjnsn4gpl0",
        "outputId": "63990dfd-5c1e-4c66-d998-578de548164f"
      },
      "outputs": [],
      "source": [
        "# --- Tratamento de Classes Desbalanceadas ---\n",
        "\n",
        "# Importa a função específica para calcular os pesos das classes da biblioteca Scikit-learn.\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# 1. Calcula os pesos para cada classe.\n",
        "# Esta função ajuda a criar pesos que serão usados para penalizar mais os erros\n",
        "# do modelo em classes com poucos exemplos durante o treinamento.\n",
        "class_weights = compute_class_weight(\n",
        "    # Parâmetro 'class_weight':\n",
        "    # Define a estratégia para calcular os pesos.\n",
        "    # 'balanced' -> Calcula os pesos de forma inversamente proporcional à frequência da classe.\n",
        "    #               Ou seja, classes com menos amostras (raras) recebem um peso maior,\n",
        "    #               e classes com muitas amostras (comuns) recebem um peso menor.\n",
        "    class_weight=\"balanced\",\n",
        "    # Parâmetro 'classes':\n",
        "    # Um array com todas as classes únicas presentes no seu dataset.\n",
        "    # Ex: [0, 1, 2] ou ['negativo', 'neutro', 'positivo'].\n",
        "    # np.unique() garante que pegamos cada classe apenas uma vez.\n",
        "    classes=np.unique(df_train[sentiment_column]),\n",
        "    # Parâmetro 'y':\n",
        "    # Um array contendo os rótulos (targets) de todos os seus dados de treino.\n",
        "    # A função usa este array para contar a frequência de cada classe\n",
        "    # e aplicar a estratégia 'balanced'.\n",
        "    y=df_train[sentiment_column].to_numpy(),\n",
        ")\n",
        "\n",
        "print(class_weights)\n",
        "\n",
        "# 2. Converte os pesos (que estão em um array numpy) para um tensor do PyTorch.\n",
        "# A função de perda do PyTorch (nn.CrossEntropyLoss) espera receber os pesos neste formato.\n",
        "# -> 'dtype=torch.float': Garante que os pesos sejam números de ponto flutuante (ex: 32-bit float).\n",
        "# -> '.to(device)': Move o tensor de pesos para o dispositivo de processamento correto (CPU ou GPU),\n",
        "weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWEcBB1wgr0W",
        "outputId": "c1eb436f-b0cf-45c2-d9ae-6136bc0bc8b7"
      },
      "outputs": [],
      "source": [
        "# Função de perda (loss function)\n",
        "# - CrossEntropyLoss é usada em tarefas de classificação multiclasse.\n",
        "# - Ele compara a previsão do modelo (outputs) com a resposta correta (targets).\n",
        "# - Calcula um único número (loss), que significa o quão \"errada\" foi a previsão.\n",
        "# - Quanto maior a perda (loss), pior foi o desempenho do modelo no batch.\n",
        "# - OBS - Caso o dataset seja desbalanceado, pode-se incluir pesos para cada classe.\n",
        "\n",
        "if CLASS_WEIGHTS:\n",
        "    # Erros em classes raras agora terão uma penalidade maior, forçando o modelo a aprendê-las.\n",
        "    print(\"Usando pesos para classes desbalanceadas.\")\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=weights).to(device)\n",
        "else:\n",
        "    print(\"Não usando pesos para as classes.\")\n",
        "    loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVH1-XbyZiAA"
      },
      "outputs": [],
      "source": [
        "# Otimizador AdamW\n",
        "# - AdamW é o otimizador recomendado para Transformers.\n",
        "# - Ele ajusta os pesos da rede neural durante o treinamento.\n",
        "# - Com base no valor da perda calculado pela (loss_fn), ele determina como ajustar cada um dos milhões de pesos\n",
        "# dentro do modelo, para que, da próxima vez, a perda seja menor. Ele implementa o algoritmo que efetivamente faz\n",
        "# o modelo aprender.\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# optimizer = AdamW(\n",
        "#     model.parameters(),\n",
        "#     lr=LEARNING_RATE,\n",
        "# )\n",
        "\n",
        "# Número total de passos (batches) durante todo o treinamento\n",
        "# Fórmula: total de lotes por época * número de épocas\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "# Passos de \"aquecimento\" (warmup) da taxa de aprendizado\n",
        "# Durante ~10% dos primeiros passos, a learning rate cresce gradualmente\n",
        "# Isso evita instabilidade no início do treino.\n",
        "# num_warmup_steps = 0\n",
        "num_warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "# Scheduler (agendador) para a taxa de aprendizado\n",
        "# - Ele controla um hiperparametro muito importante: a taxa de aprendizado (learning rate).\n",
        "# Em vez de manter essa taxa fixa, o scheduler a ajusta dinamicamente durante o treinamento\n",
        "# (aumentando no inicio e depois diminuindo), o que ajuda o modelo a convergir de forma mais rápida e estável.\n",
        "# - Começa aumentando gradualmente a learning rate (warmup)\n",
        "# - Depois decai linearmente até o final do treino\n",
        "# Esse ajuste dinâmico ajuda o modelo a convergir melhor.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,  # número de passos para \"aquecer\"\n",
        "    num_training_steps=total_steps,  # número total de passos de treino\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p91jsteMkCyP"
      },
      "source": [
        "Função auxiliar para treinar nosso modelo para uma época:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5jRaf7yZiAA"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm  # Importa a biblioteca para criar barras de progresso visuais\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "    \"\"\"\n",
        "    Executa uma época de treinamento do modelo.\n",
        "\n",
        "    Args:\n",
        "        model: O modelo a ser treinado.\n",
        "        data_loader: O DataLoader com os dados de treinamento.\n",
        "        loss_fn: A função de perda.\n",
        "        optimizer: O otimizador.\n",
        "        device: O dispositivo (GPU/CPU) para treinamento.\n",
        "        scheduler: O agendador de taxa de aprendizado.\n",
        "        n_examples: O número total de exemplos de treinamento.\n",
        "\n",
        "    Returns:\n",
        "        Uma tupla contendo a acurácia e a perda média da época.\n",
        "    \"\"\"\n",
        "    # Colocamos nosso modelo em \"modo estudo\".\n",
        "    # Isso ativa camadas como o Dropout, que ajudam a evitar que o modelo \"decore\" as respostas.\n",
        "    # É como dizer ao aluno: \"Preste atenção, é hora de aprender, não de fazer prova.\"\n",
        "    model = model.train()\n",
        "\n",
        "    # Criamos \"cadernos\" para anotar o desempenho do modelo durante a aula.\n",
        "    # 'losses' vai guardar a \"nota\" de erro de cada exercício.\n",
        "    # 'correct_predictions' vai contar quantas respostas o aluno acertou.\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    total_steps = len(data_loader)  # Total de batches (Lotes de dados) no DataLoader\n",
        "    print(f\"Total de passos - batches: {total_steps}\")\n",
        "    # Usando tqdm para a barra de progresso com uma descrição clara\n",
        "    progress_bar = tqdm(\n",
        "        enumerate(data_loader, 1),\n",
        "        total=total_steps,\n",
        "        desc=\"Iniciando Treinamento\",\n",
        "        ncols=100,\n",
        "    )\n",
        "\n",
        "    # Para cada batch (lote de dados) de dados no DataLoader\n",
        "    for batch in data_loader:\n",
        "\n",
        "        # Move os 'input_ids' e 'attention_mask' para o dispositivo correto (GPU ou CPU)\n",
        "        input_ids = batch[\"input_ids\"].to(device)  # IDs dos tokens de entrada\n",
        "        attention_mask = batch[\"attention_mask\"].to(\n",
        "            device\n",
        "        )  # Máscara de atenção para ignorar tokens de padding\n",
        "        targets = batch[\"targets\"].to(device)  # Rótulos (sentimentos)\n",
        "\n",
        "        # Passa os dados de entrada para o modelo (input_ids e attention_mask).\n",
        "        # O modelo processa cada entrada. Se um batch tiver 16 entradas, processará 16 entradas.\n",
        "        # outputs é a lista de saídas do modelo, que contém os logits para cada classe.\n",
        "        # os logits são os valores brutos de previsão para cada classe. Ainda não são probabilidades.\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,  # IDs dos tokens de entrada\n",
        "            attention_mask=attention_mask,  # Máscara de atenção para ignorar tokens de padding\n",
        "        )\n",
        "\n",
        "        # Exemplo de saída (logits) para um batch com 2 exemplos e 5 classes:\n",
        "        # tensor([\n",
        "        #   [-0.0792,  0.1209,  0.0140,  0.1531,  0.1964],\n",
        "        #   [-0.1822, -0.0655, -0.0424,  0.0147,  0.2308]\n",
        "        # ])\n",
        "        # Cada linha representa um exemplo no batch e cada coluna, um logit (pontuação) para uma classe.\n",
        "\n",
        "        # Calcula a perda para o lote atual\n",
        "        # Exemplo: se targets = tensor([4, 3]) e preds tensor([4, 4]),\n",
        "        # a loss pode ser algo como: tensor(0.47)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # Backpropagation\n",
        "        # Aqui, descobrimos EXATAMENTE onde o modelo errou em seu raciocínio.\n",
        "        # O PyTorch calcula um \"gradiente\" para cada peso do modelo, um mapa que aponta\n",
        "        # a direção e a intensidade da correção necessária para cada \"neurônio\".\n",
        "        loss.backward()\n",
        "\n",
        "        # Aplica clipping nos gradientes para evitar explosões (exploding gradients)\n",
        "        # Antes de corrigir, verificamos se o \"mapa da correção\" não é exagerado.\n",
        "        # Se a correção for grande demais (explosão de gradiente), ela pode desestabilizar o modelo.\n",
        "        # Essa linha garante que a correção seja firme, mas não destrutiva, no máximo 1.0\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # \"A Correção de Fato\"\n",
        "        # O otimizador pega o mapa da correção e ajusta os pesos (o conhecimento) do modelo.\n",
        "        # É neste exato momento que o aprendizado acontece!\n",
        "        # Atualiza os pesos do modelo com base nos gradientes calculados.\n",
        "        # Ele dá um pequeno passo na direção que deve diminuir a perda.\n",
        "        optimizer.step()\n",
        "\n",
        "        # \"Virando a Página\" (Zerando os Gradientes):\n",
        "        # Após a correção, limpamos o \"mapa\" (os gradientes).\n",
        "        # Se não fizéssemos isso, a correção do próximo batch seria somada com a do anterior,\n",
        "        # criando uma bagunça e impedindo o aprendizado correto.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # \"Ajustando a Estratégia de Ensino\" (Passo do Scheduler):\n",
        "        # O scheduler ajusta a \"intensidade\" da correção (taxa de aprendizado) para o próximo batch.\n",
        "        # Atualiza a taxa de aprendizado usando o scheduler, seguindo seu plano de aquecimento e decaimento.\n",
        "        scheduler.step()\n",
        "\n",
        "        # --- Anotando o Desempenho (Métricas - não afetam o treinamento) ---\n",
        "\n",
        "        # Obtém as classes previstas com maior valor de logit para cada exemplo\n",
        "        # Exemplo de saída: tensor([4, 4])  -> significa que o modelo previu a classe 4 para ambos\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        # Soma o número de acertos: compara a previsão com o rótulo real\n",
        "        # Exemplo: preds = tensor([4, 4]), targets = tensor([4, 3]) → 1 acerto\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "\n",
        "        # Adiciona a perda atual à lista de perdas\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Atualiza a barra de progresso com a perda média do lote\n",
        "        progress_bar.set_postfix({\"loss\": np.mean(losses)})\n",
        "        progress_bar.update()\n",
        "\n",
        "    # Após todos os batches, calcula a acurácia e a média da perda\n",
        "    accuracy = correct_predictions.double() / n_examples\n",
        "    mean_loss = np.mean(losses)\n",
        "\n",
        "    return accuracy, mean_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT9tXUt4ZiAA"
      },
      "source": [
        "O scheduler (agendador) é chamado sempre que um batch (lote) é alimentado no modelo. Evitamos a explosão de gradientes cortando os gradientes do modelo usando [clip_grad_norm_](https://pytorch.org/docs/stable/nn.html#clip-grad-norm).\n",
        "\n",
        "Vamos escrever outro que nos ajude a avaliar o modelo em um determinado carregador de dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU8t56DUZiAA"
      },
      "outputs": [],
      "source": [
        "def eval_epoch(model, data_loader, loss_fn, device, n_examples):\n",
        "    \"\"\"\n",
        "    Executa uma época de avaliação do modelo.\n",
        "\n",
        "    Args:\n",
        "        model: O modelo a ser avaliado.\n",
        "        data_loader: O DataLoader com os dados de validação ou teste.\n",
        "        loss_fn: A função de perda.\n",
        "        device: O dispositivo (GPU/CPU) para avaliação.\n",
        "        n_examples: O número total de exemplos de avaliação.\n",
        "\n",
        "    Returns:\n",
        "        Uma tupla contendo a acurácia e a perda média da época de avaliação.\n",
        "    \"\"\"\n",
        "    model = model.eval()\n",
        "\n",
        "    # Lista para armazenar as perdas e uma variável para contar as previsões corretas\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # Desativa o cálculo de gradientes para a avaliação, economizando memória\n",
        "    # e tempo de processamento.\n",
        "    with torch.no_grad():\n",
        "        # Para cada batch (lote de dados) de dados no DataLoader\n",
        "        for d in data_loader:\n",
        "            # Move os 'input_ids' e 'attention_mask' para o dispositivo correto (GPU ou CPU)\n",
        "            input_ids = d[\"input_ids\"].to(device)  # IDs dos tokens de entrada\n",
        "            attention_mask = d[\"attention_mask\"].to(\n",
        "                device\n",
        "            )  # Máscara de atenção para ignorar tokens de padding\n",
        "            targets = d[\"targets\"].to(device)  # Rótulos (sentimentos)\n",
        "\n",
        "            # Passa os dados de entrada para o modelo (input_ids e attention_mask)\n",
        "            # para obter as previsões\n",
        "            # O modelo retorna uma saída, que é uma previsão bruta (logits) para cada classe.\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,  # IDs dos tokens de entrada\n",
        "                attention_mask=attention_mask,  # Máscara de atenção para ignorar tokens de padding\n",
        "            )\n",
        "\n",
        "            # Obtém as classes previstas com maior valor de logit (probabilidade bruta)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            # Calcula a perda para o lote atual e a adiciona à lista\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # Soma o número de previsões corretas\n",
        "            correct_predictions += torch.sum(preds == targets)\n",
        "\n",
        "    # Calcula a acurácia (número de previsões corretas dividido pelo total de exemplos)\n",
        "    accuracy = correct_predictions.double() / n_examples\n",
        "    mean_loss = np.mean(losses)\n",
        "\n",
        "    # Retorna a acurácia e a perda média\n",
        "    return accuracy, mean_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjD-RJwQZiAB"
      },
      "source": [
        "Usando esses dois, podemos escrever nosso ciclo de treinamento. Também armazenaremos o histórico de treinamento.\n",
        "\n",
        "Observe que estamos armazenando o estado do melhor modelo, indicado pelo maior validation accuracy (precisão de validação)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-G17zmRZiAB",
        "outputId": "d5e75d3a-96bf-40f7-b120-75cef653a0f7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Dicionário para armazenar o histórico do treinamento (acurácia e perda)\n",
        "history = defaultdict(list)\n",
        "# Variável para armazenar a melhor acurácia de validação encontrada até o momento\n",
        "best_accuracy = 0\n",
        "\n",
        "# Loop principal que executa para cada época\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f\"Época {epoch + 1}/{EPOCHS}\")\n",
        "  print(\"-\" * 10)\n",
        "\n",
        "  # ====================\n",
        "  # FASE DE TREINAMENTO\n",
        "  # ====================\n",
        "  # Chama a função para treinar o modelo por uma época e obtém a acurácia e a perda de treino\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    len(df_train) # Passa o número total de exemplos de treinamento\n",
        "  )\n",
        "\n",
        "  print(f\"Perda de Treino: {train_loss:.4f} | Acurácia de Treino: {train_acc:.4f}\")\n",
        "\n",
        "  # ====================\n",
        "  # FASE DE VALIDAÇÃO\n",
        "  # ====================\n",
        "  # Chama a função para avaliar o modelo com os dados de validação\n",
        "  val_acc, val_loss = eval_epoch(\n",
        "      model,\n",
        "      val_data_loader,\n",
        "      loss_fn,\n",
        "      device,\n",
        "      len(df_val) # Passa o número total de exemplos de validação\n",
        "  )\n",
        "\n",
        "  print(f\"Perda de Validação: {val_loss:.4f} | Acurácia de Validação: {val_acc:.4f}\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  # ====================\n",
        "  # ARMAZENA O HISTÓRICO DE TREINAMENTO\n",
        "  # ====================\n",
        "  # Armazena a acurácia e a perda da época no histórico\n",
        "  history['train_acc'].append(train_acc)  # Adiciona a precisão de treinamento ao histórico\n",
        "  history['train_loss'].append(train_loss)  # Adiciona a perda de treinamento ao histórico\n",
        "  history['val_acc'].append(val_acc)  # Adiciona a precisão de validação ao histórico\n",
        "  history['val_loss'].append(val_loss)  # Adiciona a perda de validação ao histórico\n",
        "\n",
        "  # ====================\n",
        "  # SALVA O MELHOR MODELO (melhor acurácia de validação)\n",
        "  # ====================\n",
        "  # Esta é uma boa prática para evitar o overfitting e garantir\n",
        "  # que o melhor modelo seja utilizado.\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hpLKvsIZiAB"
      },
      "source": [
        "Visualização do Histórico de Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nt3li5o4ZiAB",
        "outputId": "cc696436-7e39-4788-9c4d-f09fbb15e370"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# VISUALIZAÇÃO DO HISTÓRICO DE TREINAMENTO\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# GRÁFICO 1: ACURÁCIA\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "train_acc = [x.cpu().item() if hasattr(x, \"cpu\") else x for x in history[\"train_acc\"]]\n",
        "val_acc = [x.cpu().item() if hasattr(x, \"cpu\") else x for x in history[\"val_acc\"]]\n",
        "\n",
        "# Cria um gráfico para visualizar a acurácia de treinamento e validação ao longo das épocas\n",
        "plt.plot(train_acc, label=\"acurácia de treino\")\n",
        "plt.plot(val_acc, label=\"acurácia de validação\")\n",
        "plt.title(\"Acurácia de Treino vs. Validação\")\n",
        "plt.ylabel(\"Acurácia\")\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.legend()\n",
        "plt.ylim([0, 1])\n",
        "plt.show()\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# GRÁFICO 2: PERDA (LOSS)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Supondo que seu dicionário 'history' contenha 'train_loss' e 'val_loss'\n",
        "train_loss = [x.cpu().item() if hasattr(x, \"cpu\") else x for x in history[\"train_loss\"]]\n",
        "val_loss = [x.cpu().item() if hasattr(x, \"cpu\") else x for x in history[\"val_loss\"]]\n",
        "\n",
        "# Cria um gráfico para visualizar a acurácia de treinamento e validação ao longo das épocas\n",
        "plt.plot(train_loss, label=\"perda de treino\")\n",
        "plt.plot(val_loss, label=\"perda de validação\")\n",
        "plt.title(\"Perda de Treino vs. Validação\")\n",
        "plt.ylabel(\"Perda\")\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ac_pSQZiAB"
      },
      "source": [
        "## AVALIAÇÃO FINAL DADOS DE TESTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNpDi0K5ZiAB"
      },
      "source": [
        "#### Obter um modelo já treinado da internet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D9pzlIQZiAB"
      },
      "outputs": [],
      "source": [
        "# !gdown 16Xq3bNEx7Owf7kOMjfgQu-BkWsX7hEpV -O best_model_state.bin\n",
        "\n",
        "# model = SentimentClassifier(len(class_names))\n",
        "# model.load_state_dict(torch.load('best_model_state.bin'))\n",
        "# model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y6SY2hRZiAC"
      },
      "source": [
        "#### Carregar o modelo salvo localmente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dipmaEJ2ZiAC",
        "outputId": "86614160-f10b-4cee-f1cc-041e03d20d4a"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# AVALIAÇÃO FINAL DO MELHOR MODELO\n",
        "# ==============================================================================\n",
        "\n",
        "# Definindo o caminho para o modelo salvo\n",
        "model_path = os.path.join(os.getcwd(), \"best_model_state.bin\")\n",
        "\n",
        "# Carregar o modelo\n",
        "model = SentimentClassifier(\n",
        "    len(class_names)\n",
        ")  # Inicialize o modelo com a mesma arquitetura\n",
        "model.load_state_dict(torch.load(model_path))  # Carregar o estado do modelo salvo\n",
        "model = model.to(device)  # Enviar o modelo para o dispositivo (GPU ou CPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJC6RlIQZiAC"
      },
      "source": [
        "#### Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7SNvgr3ZiAC",
        "outputId": "4686448b-5607-40ce-c8e1-c6bc7cd3f524"
      },
      "outputs": [],
      "source": [
        "# Vamos começar calculando a precisão dos dados de teste:\n",
        "\n",
        "test_acc, _ = eval_epoch(model, test_data_loader, loss_fn, device, len(df_test))\n",
        "\n",
        "test_acc.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-icqkeiSZiAC"
      },
      "source": [
        "Função para fazer previsões em um conjunto de dados.\n",
        "\n",
        "\n",
        "Semelhante à função de avaliação, exceto que armazenamos o texto das revisões e as probabilidades previstas (aplicando o softmax nos resultados do modelo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLuVKlIMZiAC"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "    # Define o modelo no modo de avaliação, desativando camadas como dropout e batchnorm\n",
        "    model = model.eval()\n",
        "\n",
        "    # Listas para armazenar os resultados\n",
        "    review_texts = []  # Textos dos reviews\n",
        "    predictions = []  # Classes previstas pelo modelo\n",
        "    prediction_probs = []  # Probabilidades associadas às previsões\n",
        "    real_values = []  # Valores reais (rótulos verdadeiros\n",
        "\n",
        "    # Desativa o cálculo do gradiente, economizando memória e acelerando o processo\n",
        "    with torch.no_grad():\n",
        "        # Para cada batch (lote de dados) de dados no DataLoader\n",
        "        for batch in data_loader:\n",
        "\n",
        "            texts = batch[\"review_text\"]  # Obtém os textos dos reviews\n",
        "\n",
        "            # Move os 'input_ids' e 'attention_mask' para o dispositivo correto (GPU ou CPU)\n",
        "            input_ids = batch[\"input_ids\"].to(device)  # IDs dos tokens de entrada\n",
        "            attention_mask = batch[\"attention_mask\"].to(\n",
        "                device\n",
        "            )  # Máscara de atenção para ignorar tokens de padding\n",
        "            targets = batch[\"targets\"].to(device)  # Rótulos (sentimentos)\n",
        "\n",
        "            # Passa os dados pelo modelo para obter os logits (saídas brutas)\n",
        "            # model retorna os logits, que são as previsões brutas para cada classe\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Obtém a classe prevista que possui o maior valor de logit (probabilidade bruta)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            # Aplica a função softmax nos logits para converter os valores brutos em probabilidades de classe\n",
        "            # A função `softmax` garante que a soma das probabilidades seja 1 para cada exemplo no batch\n",
        "            # `dim=1` indica que a operação softmax deve ser aplicada ao longo da segunda dimensão (as colunas), ou seja, sobre as classes\n",
        "            # Isso faz com que as previsões se tornem probabilidades, onde a classe com a maior probabilidade será a prevista\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "            # Armazena os resultados\n",
        "            review_texts.extend(texts)  # Adiciona os textos dos reviews\n",
        "            predictions.extend(preds)  # Adiciona as classes previstas\n",
        "            prediction_probs.extend(probs)  # Adiciona as probabilidades das previsões\n",
        "            real_values.extend(targets)  # Adiciona os rótulos verdadeiros\n",
        "\n",
        "    # Converte listas de tensores em um único tensor e move para a CPU\n",
        "    predictions = torch.stack(predictions).cpu()\n",
        "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "    real_values = torch.stack(real_values).cpu()\n",
        "    return review_texts, predictions, prediction_probs, real_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8VBJukJOVvs"
      },
      "source": [
        "Obtendo as previsões do modelo no conjunto de teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ch7-gU-ZiAC"
      },
      "outputs": [],
      "source": [
        "# y_review_texts: Lista contendo os textos dos reviews do conjunto de teste.\n",
        "# y_pred: Tensor com as classes previstas pelo modelo.\n",
        "# y_pred_probs: Tensor com as probabilidades associadas a cada classe.\n",
        "# y_test: Tensor com os rótulos verdadeiros.\n",
        "\n",
        "# Obtendo as previsões do modelo no conjunto de teste\n",
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "    model,  # Modelo treinado para análise de sentimentos (SentimentClassifier)\n",
        "    test_data_loader,  # DataLoader com os dados de teste, incluindo textos, input_ids e targets\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsmQIAzPZiAD"
      },
      "source": [
        "Imprime o relatório de classificação:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvw9pC33ZiAD",
        "outputId": "7e2d1fd4-f731-4ef3-90dd-9e23dff303e1"
      },
      "outputs": [],
      "source": [
        "# y_test: vetor contendo os rótulos verdadeiros.\n",
        "# y_pred: vetor contendo as classes previstas pelo modelo.\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Relatório de Classificação no Conjunto de Teste\")\n",
        "print(\"=\" * 50 + \"\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMUU7a_RZiAD"
      },
      "source": [
        "Matriz de confusão:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "gNLYrCvtZiAD",
        "outputId": "a42d022f-30a8-4b44-83cb-5c609bfb7df6"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# ANÁLISE DA MATRIZ DE CONFUSÃO\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# Cria a matriz de confusão\n",
        "def show_confusion_matrix(confusion_matrix):\n",
        "    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha=\"right\")\n",
        "    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha=\"right\")\n",
        "    plt.ylabel(\"Verdadeiro Sentimento\")\n",
        "    plt.xlabel(\"Sentimento Previsto\")\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Matriz de Confusão no Conjunto de Teste\")\n",
        "print(\"=\" * 50 + \"\\n\")\n",
        "show_confusion_matrix(df_cm)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOzcYH3xZiAD"
      },
      "source": [
        "### ANÁLISE DE UM EXEMPLO DO CONJUNTO DE TESTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "id": "ectr8sSwZiAD",
        "outputId": "5ea047f6-eed3-48bc-d4f1-f00cdfe3ed08"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# ANÁLISE DE UM EXEMPLO DO CONJUNTO DE TESTE\n",
        "# ==============================================================================\n",
        "\n",
        "# Seleciona o índice do review que deseja analisar\n",
        "idx = 10\n",
        "\n",
        "review_text = y_review_texts[idx]  # Obtém o texto do review no índice especificado\n",
        "\n",
        "# Obtém o rótulo verdadeiro (sentimento real) para esse review\n",
        "true_sentiment = y_test[idx].item()  # .item() para converter de tensor para inteiro\n",
        "\n",
        "# Cria um DataFrame com as classes e as probabilidades previstas pelo modelo para esse review\n",
        "pred_df = pd.DataFrame(\n",
        "    {\n",
        "        \"class_names\": class_names,  # Lista com os nomes das classes\n",
        "        \"values\": y_pred_probs[\n",
        "            idx\n",
        "        ].numpy(),  # Converte o tensor de probabilidades para NumPy array\n",
        "    }\n",
        ")\n",
        "\n",
        "# Exibe o texto do review, o sentimento real e as probabilidades previstas\n",
        "print(f\"Review: {review_text}\")\n",
        "print(\n",
        "    f\"Sentimento Real: {class_names[true_sentiment]}\"\n",
        ")  # Mostra o nome da classe em vez do índice\n",
        "print(pred_df)\n",
        "\n",
        "\n",
        "# Cria o gráfico de barras com as probabilidades\n",
        "sns.barplot(x=\"values\", y=\"class_names\", data=pred_df, orient=\"h\")\n",
        "plt.ylabel(\"Sentimento\")\n",
        "plt.xlabel(\"Probabilidade\")\n",
        "plt.xlim([0, 1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mLIuhYUZiAE"
      },
      "source": [
        "### PREVISÃO EM TEXTO BRUTO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSAIA1GtZiAE"
      },
      "outputs": [],
      "source": [
        "review_text = \"O aplicativo IDR-Peixe muito bom, recomendo a todos.\"\n",
        "\n",
        "encoded_review = tokenizer.encode_plus(\n",
        "    review_text,\n",
        "    max_length=MAX_LEN,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=False,\n",
        "    padding=\"max_length\",  # Changed from pad_to_max_length=True\n",
        "    return_attention_mask=True,\n",
        "    return_tensors=\"pt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXIpZX7SZiAE"
      },
      "source": [
        "Vamos obter as previsões do nosso modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nySzTD8ZiAE",
        "outputId": "c95f3d86-47e6-4aab-d8ef-e05e337a4443"
      },
      "outputs": [],
      "source": [
        "input_ids = encoded_review[\"input_ids\"].to(device)\n",
        "attention_mask = encoded_review[\"attention_mask\"].to(device)\n",
        "\n",
        "output = model(input_ids, attention_mask)\n",
        "_, prediction = torch.max(output, dim=1)\n",
        "\n",
        "print(f\"Review text: {review_text}\")\n",
        "print(f\"Sentiment  : {class_names[prediction]}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
